{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e37ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 15:52:13.893783: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 15:52:17.476004: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/scratch/ponel/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-05-25 15:52:17.476276: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/scratch/ponel/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-05-25 15:52:17.476300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a303e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541f0ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 15:52:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 22%   31C    P2    58W / 250W |    445MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 22%   29C    P2    46W / 250W |    445MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:DA:00.0 Off |                  N/A |\n",
      "| 22%   24C    P8    17W / 250W |    335MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 22%   24C    P8    21W / 250W |    335MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021e0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9baffd5",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9b818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim_x = 28\n",
    "img_dim_y = 28\n",
    "img_dim_z = 1\n",
    "\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9ea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(dirpath='/data/project/FoolingDetection/mnist.npz'):\n",
    "    with np.load(dirpath) as data:\n",
    "        x_train, y_train = data['x_train'], data['y_train']\n",
    "        x_test, y_test = data['x_test'], data['y_test']\n",
    "\n",
    "    x_train = x_train/255\n",
    "    x_test = x_test/255\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f497e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\"Load a single batch of the CIFAR-10 dataset\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(dirpath=\"/data/project/FoolingDetection/cifar-10-batches-py\"):\n",
    "    \"\"\"Load all batches of the CIFAR-10 dataset from a directory\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 5):\n",
    "        f = os.path.join(dirpath, 'data_batch_%d' % (b,))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(dirpath, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad03eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_MNIST()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01070571",
   "metadata": {},
   "source": [
    "## Stop early!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b7dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\",\n",
    "  patience=3,  \n",
    "  restore_best_weights=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da607d",
   "metadata": {},
   "source": [
    "## Some Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08736405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "# Instantiate an optimizer, initializer, regularizer, loss_fn\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "regularizer = tf.keras.regularizers.L2(1e-4)\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f8db7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a1827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Define a distribution strategy that uses only GPUs 0 and 1\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "\n",
    "def get_model_raw():\n",
    "    with strategy.scope():\n",
    "        ### Hyperparameters\n",
    "        # Instantiate an optimizer, initializer, regularizer, loss_fn\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        initializer = tf.keras.initializers.HeNormal()\n",
    "        regularizer = tf.keras.regularizers.L2(1e-4)\n",
    "        loss_fn = tf.losses.SparseCategoricalCrossentropy()\n",
    "        metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        # Alternative model from kaggle:\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(img_dim_x, (3,3), padding='same', activation='relu', input_shape=(img_dim_x,img_dim_y,img_dim_z)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(img_dim_x, (3,3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "        model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(10))\n",
    "        model.add(layers.Activation('softmax'))\n",
    "        \n",
    "        model.compile(loss=loss_fn, optimizer='adam', metrics=[metrics])\n",
    "        return model\n",
    "model = get_model_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc53363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 15:52:38.801940: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 24s 27ms/step - loss: 0.3967 - sparse_categorical_accuracy: 0.8799 - val_loss: 1.5189 - val_sparse_categorical_accuracy: 0.5548\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9721 - val_loss: 0.0262 - val_sparse_categorical_accuracy: 0.9911\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0663 - sparse_categorical_accuracy: 0.9798 - val_loss: 0.0215 - val_sparse_categorical_accuracy: 0.9927\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0238 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.0254 - val_sparse_categorical_accuracy: 0.9923\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0436 - sparse_categorical_accuracy: 0.9864 - val_loss: 0.0223 - val_sparse_categorical_accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7258b03730>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_train, y_train, epochs=2, batch_size=128, validation_data=(x_test,y_test), callbacks=callbacks)\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test,y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb181bad",
   "metadata": {},
   "source": [
    "# Comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197dead",
   "metadata": {},
   "source": [
    "## Naive approach: Take two images (or heatmap) (with same dimensions) and calculate different similarity metrics for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48dfaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6b44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(im1, im2):\n",
    "    im1 = im1.flatten()\n",
    "    im2 = im2.flatten()\n",
    "    return np.linalg.norm(im1 - im2)\n",
    "\n",
    "def manhattan_distance(im1, im2):\n",
    "    im1 = im1.flatten()\n",
    "    im2 = im2.flatten()\n",
    "    return distance.cityblock(im1, im2)\n",
    "  \n",
    "def minkowski_distance(im1, im2, p): # p=1 <-> Manhatten, p=2 Euclidian\n",
    "    im1 = im1.flatten()\n",
    "    im2 = im2.flatten()\n",
    "    return distance.minkowski(im1, im2, p)\n",
    "\n",
    "def chebyshev_distance(im1, im2):\n",
    "    im1 = im1.flatten()\n",
    "    im2 = im2.flatten()\n",
    "    return distance.chebyshev(im1, im2)\n",
    "\n",
    "def cosine_sim(im1, im2):\n",
    "    im1 = im1.flatten()\n",
    "    im2 = im2.flatten()\n",
    "    return cosine_similarity(im1.reshape(1, -1), im2.reshape(1, -1))\n",
    "\n",
    "def compute_ssim(im1, im2, multichannel=True):\n",
    "    return ssim(im1, im2, multichannel=multichannel)\n",
    "\n",
    "def run_all_distance_metrics(p1, p2):\n",
    "  minkowski_p = 3\n",
    "  return {\"euclidean:\": euclidean_distance(p1,p2),\n",
    "          \"manhatten:\": manhattan_distance(p1,p2),\n",
    "          \"minkowski-{}\".format(minkowski_p): minkowski_distance(p1, p2, minkowski_p),\n",
    "          \"chebyshev\": chebyshev_distance(p1,p2),\n",
    "          \"cosine sim\" : cosine_sim(p1,p2)\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad48217",
   "metadata": {},
   "source": [
    "# Running various techniques on more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1c433",
   "metadata": {},
   "source": [
    "## Run the different explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d74cf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dict with key=class values=data\n",
    "def create_dict_classes_datapoints(x_test, y_test):\n",
    "    class_dict = {}\n",
    "    for i in range(len(x_test)):\n",
    "        key = y_test[i]\n",
    "        if key in class_dict:\n",
    "            class_dict[key].append(x_test[i])\n",
    "        else:\n",
    "            class_dict[key] = [x_test[i]]\n",
    "    return class_dict\n",
    "\n",
    "# # Define sorted labels \n",
    "# unique_sorted_labels = sorted(set(map(str, y_test)), key=lambda x: (not x.isdigit(), int(x) if x.isdigit() else x))\n",
    "# images_by_class = create_dict_classes_datapoints(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287e6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_explainers(x_in, y_in, model, NUM_IMAGES=300):\n",
    "\n",
    "    explainer_grad = GradCAM()\n",
    "    grids_gradcam = []\n",
    "    grids_gradcam_guided = []\n",
    "    explainer_ig = IntegratedGradients()\n",
    "    grids_ig = []\n",
    "    explainer_g_t_i = GradientsInputs()\n",
    "    grids_g_t_i = []\n",
    "    explainer_vgrads = VanillaGradients()\n",
    "    grids_vgrads = []\n",
    "    explainer_occlusion = OcclusionSensitivity()\n",
    "    grids_occlusion = []\n",
    "\n",
    "    # Iterate over the test_dataset for NUM_IMAGES. Run all the explanations using tf-explain and store the results in lists to be accessed later\n",
    "    for index, (x_example, y_example) in enumerate(zip(x_in, y_in)):\n",
    "      if index == NUM_IMAGES:\n",
    "        break\n",
    "      print(\"image: {}\".format(index))\n",
    "      x_example = ([x_example], None)\n",
    "      y_example_index = int(y_example)\n",
    "\n",
    "      grids_gradcam.append(explainer_grad.explain(x_example, model, class_index=y_example_index, use_guided_grads=False))\n",
    "      grids_gradcam_guided.append(explainer_grad.explain(x_example, model, class_index=y_example_index, use_guided_grads=True))\n",
    "      grids_ig.append(explainer_ig.explain(x_example, model, class_index=y_example_index))\n",
    "      grids_g_t_i.append(explainer_g_t_i.explain(x_example, model, class_index=y_example_index))\n",
    "      grids_vgrads.append(explainer_vgrads.explain(x_example, model, class_index=y_example_index))\n",
    "      grids_occlusion.append(explainer_occlusion.explain(x_example, model, class_index=y_example_index, patch_size=7))\n",
    "    \n",
    "    return [(grids_gradcam, \"gradcam\"), \n",
    "              (grids_gradcam_guided, \"gradcam_guided\"), \n",
    "              (grids_ig, \"ig\"), \n",
    "              (grids_g_t_i, \"g_t_i\"), \n",
    "              (grids_vgrads, \"vgrads\"), \n",
    "              (grids_occlusion, \"occlusion\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea82ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now store the grids on the local disk so we do not have to re-run this everytime\n",
    "\n",
    "\n",
    "# grid_list needs to match definiton in run_explainations()\n",
    "def save_grids_to_disk(grids_list, save_dir, dataset, model, attack=\"correct\", override=False):\n",
    "\n",
    "    # Define save directory\n",
    "    save_dir = os.path.join(save_dir, dataset, model, attack)\n",
    "    \n",
    "    if os.path.exists(save_dir) and not override:\n",
    "        raise Exception(\"The directory already exists and `override=False` was passed. Please make sure that you want to do this, this will delete any previous results.\")\n",
    "    \n",
    "    # Iterate over all grids and save them to corresponding subfolder\n",
    "    for grids, subfolder in grids_list:\n",
    "        subfolder_path = os.path.join(save_dir, subfolder)\n",
    "\n",
    "        # Create subdirectory if it does not exist\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "\n",
    "        # Save each grid as an image\n",
    "        for i, grid in enumerate(grids):\n",
    "            plt.imshow(grid, cmap='hot', interpolation='nearest')\n",
    "            plt.savefig(os.path.join(subfolder_path, f\"{i}.png\"))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995d2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to run the metrics which supports batches of images\n",
    "def run_all_distance_metrics_batch(P1, P2):\n",
    "    minkowski_p = 3\n",
    "    n = len(P1)\n",
    "    result = {\"euclidean\": 0,\n",
    "              \"manhattan\": 0,\n",
    "              \"minkowski-{}\".format(minkowski_p): 0,\n",
    "              \"chebyshev\": 0,\n",
    "              \"cosine sim\": 0}\n",
    "    for p1, p2 in zip(P1, P2):\n",
    "        result[\"euclidean\"] += euclidean_distance(p1,p2)\n",
    "        result[\"manhattan\"] += manhattan_distance(p1,p2)\n",
    "        result[\"minkowski-{}\".format(minkowski_p)] += minkowski_distance(p1, p2, minkowski_p)\n",
    "        result[\"chebyshev\"] += chebyshev_distance(p1,p2)\n",
    "        result[\"cosine sim\"] += cosine_sim(p1,p2)\n",
    "    for key in result:\n",
    "        result[key] /= n\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123f479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
