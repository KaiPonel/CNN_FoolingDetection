{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3542fb2c",
   "metadata": {},
   "source": [
    "This notebooks purpose is to train models. The trained models weights are then being saved and can be used in any other notebook. This is done for two reasons:\n",
    "1) Save time\n",
    "2) Gurantee consistency between the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16b39a",
   "metadata": {},
   "source": [
    "# Imports (Global, Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436f07cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 14:57:04.749011: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 14:57:08.027948: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/scratch/ponel/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-06-17 14:57:08.028283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/scratch/ponel/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-06-17 14:57:08.028323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3d6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.ModelProvider import get_model \n",
    "from lib.DataProvider import load_MNIST, load_CIFAR10\n",
    "from lib.ConfigurationProvider import get_EarlyStopping\n",
    "from lib.FeatureProvider import safe_model_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b168dd0",
   "metadata": {},
   "source": [
    "# Training CNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09207775",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b96d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 13:59:06.619498: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 19s 20ms/step - loss: 0.4275 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.2931 - val_sparse_categorical_accuracy: 0.8994\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.1004 - sparse_categorical_accuracy: 0.9698 - val_loss: 0.0354 - val_sparse_categorical_accuracy: 0.9875\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0748 - sparse_categorical_accuracy: 0.9778 - val_loss: 0.0248 - val_sparse_categorical_accuracy: 0.9909\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0317 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 5/100\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9841 - val_loss: 0.0238 - val_sparse_categorical_accuracy: 0.9923\n",
      "Epoch 6/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0474 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.0217 - val_sparse_categorical_accuracy: 0.9929\n",
      "Epoch 7/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0434 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9940\n",
      "Epoch 8/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 9/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 0.9940\n",
      "Epoch 10/100\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0348 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0154 - val_sparse_categorical_accuracy: 0.9948\n",
      "Epoch 11/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0173 - val_sparse_categorical_accuracy: 0.9940\n",
      "Epoch 12/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0191 - val_sparse_categorical_accuracy: 0.9944\n",
      "Epoch 13/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0152 - val_sparse_categorical_accuracy: 0.9948\n",
      "Epoch 14/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9913 - val_loss: 0.0160 - val_sparse_categorical_accuracy: 0.9951\n",
      "Epoch 15/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9929 - val_loss: 0.0168 - val_sparse_categorical_accuracy: 0.9947\n",
      "Epoch 16/100\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0168 - val_sparse_categorical_accuracy: 0.9943\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_MNIST()\n",
    "model_cnn_mnist = get_model(model_type=\"cnn\", dataset=\"mnist\")\n",
    "model_cnn_mnist.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test,y_test),callbacks=[get_EarlyStopping()])\n",
    "safe_model_weights(model=model_cnn_mnist, model_name=\"cnn\", dataset=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59556300",
   "metadata": {},
   "source": [
    "## Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5232e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 14:08:11.581794: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 16s 24ms/step - loss: 2.0037 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.7218 - val_sparse_categorical_accuracy: 0.3949\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.4337 - sparse_categorical_accuracy: 0.4924 - val_loss: 1.2114 - val_sparse_categorical_accuracy: 0.5642\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 1.1754 - sparse_categorical_accuracy: 0.5827 - val_loss: 1.4289 - val_sparse_categorical_accuracy: 0.5291\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.0230 - sparse_categorical_accuracy: 0.6361 - val_loss: 1.3612 - val_sparse_categorical_accuracy: 0.5257\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.9270 - sparse_categorical_accuracy: 0.6736 - val_loss: 0.8203 - val_sparse_categorical_accuracy: 0.7103\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.8570 - sparse_categorical_accuracy: 0.6993 - val_loss: 0.8156 - val_sparse_categorical_accuracy: 0.7178\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.8038 - sparse_categorical_accuracy: 0.7174 - val_loss: 0.8184 - val_sparse_categorical_accuracy: 0.7258\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.7650 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.7272 - val_sparse_categorical_accuracy: 0.7437\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.7212 - sparse_categorical_accuracy: 0.7502 - val_loss: 0.7380 - val_sparse_categorical_accuracy: 0.7511\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.6899 - sparse_categorical_accuracy: 0.7589 - val_loss: 0.6676 - val_sparse_categorical_accuracy: 0.7741\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.6674 - sparse_categorical_accuracy: 0.7696 - val_loss: 0.6478 - val_sparse_categorical_accuracy: 0.7748\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.6396 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.6476 - val_sparse_categorical_accuracy: 0.7839\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.6177 - sparse_categorical_accuracy: 0.7844 - val_loss: 0.7084 - val_sparse_categorical_accuracy: 0.7619\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.5944 - sparse_categorical_accuracy: 0.7940 - val_loss: 0.6629 - val_sparse_categorical_accuracy: 0.7752\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.5800 - sparse_categorical_accuracy: 0.7995 - val_loss: 0.5810 - val_sparse_categorical_accuracy: 0.8054\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.5603 - sparse_categorical_accuracy: 0.8050 - val_loss: 0.5869 - val_sparse_categorical_accuracy: 0.8033\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.5506 - sparse_categorical_accuracy: 0.8100 - val_loss: 0.5726 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.5277 - sparse_categorical_accuracy: 0.8173 - val_loss: 0.6201 - val_sparse_categorical_accuracy: 0.7928\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.5192 - sparse_categorical_accuracy: 0.8213 - val_loss: 0.6063 - val_sparse_categorical_accuracy: 0.7968\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.5013 - sparse_categorical_accuracy: 0.8274 - val_loss: 0.5158 - val_sparse_categorical_accuracy: 0.8267\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4939 - sparse_categorical_accuracy: 0.8295 - val_loss: 0.5612 - val_sparse_categorical_accuracy: 0.8143\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4820 - sparse_categorical_accuracy: 0.8328 - val_loss: 0.5085 - val_sparse_categorical_accuracy: 0.8313\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4720 - sparse_categorical_accuracy: 0.8356 - val_loss: 0.5533 - val_sparse_categorical_accuracy: 0.8155\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4629 - sparse_categorical_accuracy: 0.8375 - val_loss: 0.5193 - val_sparse_categorical_accuracy: 0.8302\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4501 - sparse_categorical_accuracy: 0.8431 - val_loss: 0.4887 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4443 - sparse_categorical_accuracy: 0.8469 - val_loss: 0.5596 - val_sparse_categorical_accuracy: 0.8150\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4333 - sparse_categorical_accuracy: 0.8483 - val_loss: 0.5757 - val_sparse_categorical_accuracy: 0.8097\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.4318 - sparse_categorical_accuracy: 0.8516 - val_loss: 0.5194 - val_sparse_categorical_accuracy: 0.8281\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_CIFAR10()\n",
    "model_cnn_cifar10 = get_model(model_type=\"cnn\", dataset=\"cifar10\")\n",
    "model_cnn_cifar10.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test,y_test),callbacks=[get_EarlyStopping()])\n",
    "safe_model_weights(model=model_cnn_cifar10, model_name=\"cnn\", dataset=\"cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e34258",
   "metadata": {},
   "source": [
    "# Training MLP Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562f6f1",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d03bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "469/469 [==============================] - 6s 7ms/step - loss: 0.2704 - sparse_categorical_accuracy: 0.9223 - val_loss: 0.1239 - val_sparse_categorical_accuracy: 0.9606\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0987 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.0888 - val_sparse_categorical_accuracy: 0.9724\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9797 - val_loss: 0.0855 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0481 - sparse_categorical_accuracy: 0.9848 - val_loss: 0.0805 - val_sparse_categorical_accuracy: 0.9747\n",
      "Epoch 5/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0732 - val_sparse_categorical_accuracy: 0.9775\n",
      "Epoch 6/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0292 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0705 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 7/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0674 - val_sparse_categorical_accuracy: 0.9819\n",
      "Epoch 8/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0746 - val_sparse_categorical_accuracy: 0.9789\n",
      "Epoch 9/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0799 - val_sparse_categorical_accuracy: 0.9782\n",
      "Epoch 10/100\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0794 - val_sparse_categorical_accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_MNIST()\n",
    "model_cnn_mnist = get_model(model_type=\"mlp\", dataset=\"mnist\")\n",
    "model_cnn_mnist.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test,y_test),callbacks=[get_EarlyStopping()])\n",
    "safe_model_weights(model=model_cnn_mnist, model_name=\"mlp\", dataset=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf5ab2",
   "metadata": {},
   "source": [
    "## Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c996bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 4s 9ms/step - loss: 50.5205 - sparse_categorical_accuracy: 0.1999 - val_loss: 13.2812 - val_sparse_categorical_accuracy: 0.2226\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 8.8937 - sparse_categorical_accuracy: 0.2306 - val_loss: 6.8693 - val_sparse_categorical_accuracy: 0.2011\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.7467 - sparse_categorical_accuracy: 0.2462 - val_loss: 3.7572 - val_sparse_categorical_accuracy: 0.2406\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 3.2513 - sparse_categorical_accuracy: 0.2513 - val_loss: 2.4179 - val_sparse_categorical_accuracy: 0.2914\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 2.6659 - sparse_categorical_accuracy: 0.2684 - val_loss: 2.3133 - val_sparse_categorical_accuracy: 0.2851\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 2.2446 - sparse_categorical_accuracy: 0.2963 - val_loss: 2.2288 - val_sparse_categorical_accuracy: 0.2744\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 2.0293 - sparse_categorical_accuracy: 0.3221 - val_loss: 1.9170 - val_sparse_categorical_accuracy: 0.3491\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.9284 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.9633 - val_sparse_categorical_accuracy: 0.3219\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.8746 - sparse_categorical_accuracy: 0.3465 - val_loss: 1.8834 - val_sparse_categorical_accuracy: 0.3535\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.8377 - sparse_categorical_accuracy: 0.3558 - val_loss: 2.0720 - val_sparse_categorical_accuracy: 0.3105\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.8552 - sparse_categorical_accuracy: 0.3526 - val_loss: 1.9361 - val_sparse_categorical_accuracy: 0.3465\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.7916 - sparse_categorical_accuracy: 0.3699 - val_loss: 1.8200 - val_sparse_categorical_accuracy: 0.3637\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.7870 - sparse_categorical_accuracy: 0.3692 - val_loss: 1.7842 - val_sparse_categorical_accuracy: 0.3679\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.7939 - sparse_categorical_accuracy: 0.3647 - val_loss: 1.7902 - val_sparse_categorical_accuracy: 0.3597\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.7938 - sparse_categorical_accuracy: 0.3688 - val_loss: 1.8442 - val_sparse_categorical_accuracy: 0.3363\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.8013 - sparse_categorical_accuracy: 0.3620 - val_loss: 1.8455 - val_sparse_categorical_accuracy: 0.3585\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_CIFAR10()\n",
    "model_cnn_mnist = get_model(model_type=\"mlp\", dataset=\"cifar10\")\n",
    "model_cnn_mnist.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test,y_test),callbacks=[get_EarlyStopping()])\n",
    "safe_model_weights(model=model_cnn_mnist, model_name=\"mlp\", dataset=\"cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e16b4",
   "metadata": {},
   "source": [
    "# ImageNet (Pretrained) Model\n",
    "(ToDo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
